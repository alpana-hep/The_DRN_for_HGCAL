#!/bin/bash -l

#SBATCH --time=24:00:00
#SBATCH --ntasks=4
#SBATCH --mem=350g
#SBATCH --mail-type=ALL
#SBATCH --mail-user=asirohi@umn.edu
#SBATCH -p cms-1,v100
#SBATCH --gres=gpu:v100:1
#SBATCH --job-name="lr_0.0003_trainign"

folder="/home/rusack/asirohi/public/finalGNN/output_training/withFulldataset/scaled_MC/epoch9_onwards"
#v2->trueE, no semi
#v1->semi_dscb, loss fun= dscb

export PYTHONUNBUFFERED=1
module load cuda/10.1
module load cmake
module load gcc
module load graphviz
source activate torch1.8
./../../train $folder HGCAL_TestBeam/test_0to5M_scaled_/ --idx_name all  --best_arch --nosemi --target trueE --lr_sched Const --max_lr 0.0003 --valid_batch_size 400  --train_batch_size 450 --acc_rate 7 --warm  ./../../output_training/withFulldataset/scaled_MC/epoch6_onwards/checkpoints/model_checkpoint_DynamicReductionNetwork_264709_406b0b4142_asirohi_002.pth.tar  &>> $folder/training.log
#train $folder <DATA FOLDER> --arguments... &>> $folder/training.log
#<DATA FOLDER> options include:
#2018_UL_Particle_MC_30M: latest large dataset for electrons
#2018_UL_Photon_Refined: latest large dataset for photons
#2018_UL_photon_Refined/test: test version of the above with only ~8k particles
